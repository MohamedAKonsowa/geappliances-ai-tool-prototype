<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Security-Policy" content="default-src 'none'; img-src data:; style-src 'unsafe-inline'; script-src 'unsafe-inline'; connect-src 'self'; base-uri 'none'; form-action 'none;'>"/>
<title>Story Generator App</title>
<style>
body {
    font-family: Arial, sans-serif;
}

#prompt-input-field {
    width: 50%;
    height: 200px;
    padding: 10px;
    border-radius: 5px;
    border: 1px solid #ccc;
    margin-bottom: 20px;
}

#story-display-area {
    width: 50%;
    height: 300px;
    padding: 10px;
    border-radius: 5px;
    border: 1px solid #ccc;
}
</style>
</head>
<body>

<div id="prompt-input-field"></div>
<div id="story-display-area" style="display:none;"></div>

<script>
let geaRuntimeLLM = null;

function generateStory(prompt) {
    // implement LLM story generation logic here
}

async function handleSubmit() {
    const prompt = document.getElementById('prompt-input-field').value;
    const story = await generateStory(prompt);
    document.getElementById('story-display-area').innerHTML = story;
}

document.addEventListener('DOMContentLoaded', async () => {
    const geaRuntimeLLM = window.geaRuntimeLLM;
    geaRuntimeLLM(prompt='Enter a prompt:');
});

function handleBack() {
    location.href = 'index.html';
}
</script>


<script id="gea-runtime-helper">
(function () {
  if (window.geaRuntimeLLM) return;
  const defaultModel = "llama3.2";
  async function callRuntimeLLM(prompt, options = {}) {
    if (!prompt || typeof prompt !== "string") {
      throw new Error("Prompt must be a non-empty string");
    }
    const model = typeof options.model === 'string' && options.model.trim() ? options.model.trim() : defaultModel;
    const response = await fetch('/api/runtime/llm', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ prompt, model }),
      signal: options.signal
    });
    if (!response.ok) {
      const error = await response.json().catch(() => ({}));
      throw new Error(error && error.error ? error.error : 'Runtime LLM request failed');
    }
    const data = await response.json().catch(() => ({}));
    return data && data.response ? data.response : '';
  }
  window.geaRuntimeLLM = callRuntimeLLM;
})();
</script>
</body>
</html>